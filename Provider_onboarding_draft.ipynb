{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxZQlX59U4EQjGMKwyv/9E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghoshmoumita04/ProviderOnboarding/blob/main/Provider_onboarding_draft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1mymX7c4L9s",
        "outputId": "26518ab1-79be-4f67-a9a1-d62c5ab5063e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import base64\n",
        "import datetime\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ],
      "metadata": {
        "id": "OY1JXmC44WZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ATTACHMENT_DIR = \"attachments\"\n",
        "AUDIT_LOG_DIR = \"audit_logs\"\n",
        "\n",
        "os.makedirs(ATTACHMENT_DIR, exist_ok=True)\n",
        "os.makedirs(AUDIT_LOG_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Directories ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbRwC0944a8Y",
        "outputId": "26cd1d1d-f262-4fc6-d871-f4c99fcc45bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directories ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def log_audit(request_id: str, provider_name: str, action: str, details: dict):\n",
        "    log_entry = {\n",
        "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "        \"request_id\": request_id,\n",
        "        \"provider_name\": provider_name,\n",
        "        \"action\": action,\n",
        "        \"details\": details\n",
        "    }\n",
        "\n",
        "    logfile = f\"{AUDIT_LOG_DIR}/{request_id}.json\"\n",
        "\n",
        "    if os.path.exists(logfile):\n",
        "        with open(logfile, \"r\") as f:\n",
        "            logs = json.load(f)\n",
        "    else:\n",
        "        logs = []\n",
        "\n",
        "    logs.append(log_entry)\n",
        "\n",
        "    with open(logfile, \"w\") as f:\n",
        "        json.dump(logs, f, indent=2)\n"
      ],
      "metadata": {
        "id": "hWz-qBxF4cTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttachmentHandler:\n",
        "\n",
        "    def save_small_attachments(self, small_attachments: List[Dict]) -> List[str]:\n",
        "        saved = []\n",
        "        for att in small_attachments:\n",
        "            path = f\"{ATTACHMENT_DIR}/{att['file_name']}\"\n",
        "            with open(path, \"wb\") as f:\n",
        "                f.write(base64.b64decode(att[\"content_base64\"]))\n",
        "            saved.append(path)\n",
        "        return saved\n",
        "\n",
        "    def process_all(self, small_attachments, large_attachments):\n",
        "        files = []\n",
        "        if small_attachments:\n",
        "            files += self.save_small_attachments(small_attachments)\n",
        "\n",
        "        if large_attachments:\n",
        "            print(\"Large attachment download skipped in Colab (handled via SN in prod)\")\n",
        "\n",
        "        return files\n"
      ],
      "metadata": {
        "id": "fuXh0oOH4hON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_documents(files: List[str]) -> str:\n",
        "    text = \"\"\n",
        "    for file in files:\n",
        "        with open(file, \"r\", errors=\"ignore\") as f:\n",
        "            text += f.read() + \"\\n\"\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "_JGyjuHy4q6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_provider_data(text):\n",
        "    data = {}\n",
        "\n",
        "    def find(pattern):\n",
        "        m = re.search(pattern, text)\n",
        "        return m.group(1) if m else None\n",
        "\n",
        "    data[\"provider_name\"] = find(r\"Provider Name:\\s*(.+)\")\n",
        "    data[\"npi\"] = find(r\"NPI:\\s*(\\d+)\")\n",
        "    data[\"license_number\"] = find(r\"License Number:\\s*(\\S+)\")\n",
        "    data[\"license_state\"] = find(r\"License State:\\s*(\\S+)\")\n",
        "    data[\"license_expiry\"] = find(r\"License Expiry:\\s*(\\S+)\")\n",
        "    data[\"tax_id\"] = find(r\"Tax ID:\\s*(\\S+)\")\n",
        "    data[\"specialty\"] = find(r\"Specialty:\\s*(.+)\")\n",
        "\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "StQT-V574svy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_npi(npi):\n",
        "    return npi and len(npi) == 10\n",
        "\n",
        "def verify_license(expiry):\n",
        "    return expiry and expiry > \"2025-01-01\"\n",
        "\n",
        "def oig_exclusion_check(npi):\n",
        "    return False  # External API in real life\n",
        "\n",
        "def validate_provider(data):\n",
        "    issues = []\n",
        "    if not verify_npi(data.get(\"npi\")):\n",
        "        issues.append(\"Invalid NPI\")\n",
        "    if not verify_license(data.get(\"license_expiry\")):\n",
        "        issues.append(\"License expired\")\n",
        "    if oig_exclusion_check(data.get(\"npi\")):\n",
        "        issues.append(\"OIG exclusion\")\n",
        "    return issues\n"
      ],
      "metadata": {
        "id": "6wbjq97K4wkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_risk_features(validation_issues, name_similarity, sanctions_flag):\n",
        "    \"\"\"\n",
        "    Converts provider compliance signals into ML-ready features\n",
        "    \"\"\"\n",
        "    return np.array([[\n",
        "        len(validation_issues),          # Number of validation failures\n",
        "        int(name_similarity < 0.85),     # Name mismatch flag\n",
        "        int(sanctions_flag)              # Regulatory risk flag\n",
        "    ]])\n",
        "def predict_provider_risk(validation_issues, name_similarity, sanctions_flag):\n",
        "    features = build_risk_features(\n",
        "        validation_issues,\n",
        "        name_similarity,\n",
        "        sanctions_flag\n",
        "    )\n",
        "    return risk_model.predict(features)[0]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LRV9cqIG40Ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# alternative to test\n",
        "X_train = np.array([\n",
        "    [0, 0, 0],\n",
        "    [1, 0, 0],\n",
        "    [1, 1, 0],\n",
        "    [2, 1, 1]\n",
        "])\n",
        "y_train = np.array([0, 0, 1, 1])\n",
        "\n",
        "risk_model = RandomForestClassifier()\n",
        "risk_model.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "1i1SYkG96Ho7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_risk_features(issues, name_mismatch):\n",
        "    return np.array([[len(issues), name_mismatch, 0]])"
      ],
      "metadata": {
        "id": "ugC3BXof6Oj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def onboarding_decision(risk, issues):\n",
        "    if risk == 0 and not issues:\n",
        "        return \"AUTO_APPROVED\"\n",
        "    return \"MANUAL_REVIEW\"\n"
      ],
      "metadata": {
        "id": "MDUSrGS45Exv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AIProcessor:\n",
        "\n",
        "    def process(self, request_id, provider_data, attachment_files):\n",
        "        text = read_documents(attachment_files)\n",
        "        extracted = extract_provider_data(text)\n",
        "\n",
        "        issues = validate_provider(extracted)\n",
        "\n",
        "        name_mismatch = 0 if extracted.get(\"provider_name\") == provider_data.get(\"provider_name\") else 1\n",
        "        features = generate_risk_features(issues, name_mismatch)\n",
        "\n",
        "        risk = risk_model.predict(features)[0]\n",
        "        decision = onboarding_decision(risk, issues)\n",
        "\n",
        "        return {\n",
        "            \"decision\": decision,\n",
        "            \"risk\": int(risk),\n",
        "            \"issues\": issues,\n",
        "            \"extracted_data\": extracted\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Bq5s96rO5FTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_provider_record(provider_data, ai_result):\n",
        "    if ai_result[\"decision\"] != \"AUTO_APPROVED\":\n",
        "        return {\"status\": \"SKIPPED\"}\n",
        "\n",
        "    record = {\n",
        "        \"provider_id\": provider_data[\"provider_id\"],\n",
        "        \"provider_name\": provider_data[\"provider_name\"],\n",
        "        \"created_at\": datetime.datetime.now().isoformat(),\n",
        "        \"status\": \"ACTIVE\"\n",
        "    }\n",
        "\n",
        "    return {\"status\": \"CREATED\", \"record\": record}\n"
      ],
      "metadata": {
        "id": "M7y8ZyVd5JkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_provider_request(payload):\n",
        "    request_id = payload[\"request_id\"]\n",
        "    provider_data = payload[\"provider_data\"]\n",
        "\n",
        "    log_audit(request_id, provider_data[\"provider_name\"], \"REQUEST_RECEIVED\", payload)\n",
        "\n",
        "    handler = AttachmentHandler()\n",
        "    files = handler.process_all(\n",
        "        payload.get(\"small_attachments\", []),\n",
        "        payload.get(\"large_attachments\", [])\n",
        "    )\n",
        "\n",
        "    log_audit(request_id, provider_data[\"provider_name\"], \"ATTACHMENTS_READY\", files)\n",
        "\n",
        "    ai = AIProcessor()\n",
        "    ai_result = ai.process(request_id, provider_data, files)\n",
        "\n",
        "    log_audit(request_id, provider_data[\"provider_name\"], \"AI_COMPLETED\", ai_result)\n",
        "\n",
        "    provider_insert = create_provider_record(provider_data, ai_result)\n",
        "    log_audit(request_id, provider_data[\"provider_name\"], \"PROVIDER_TABLE_UPDATE\", provider_insert)\n",
        "\n",
        "    return {\n",
        "        \"request_id\": request_id,\n",
        "        \"decision\": ai_result[\"decision\"],\n",
        "        \"issues\": ai_result[\"issues\"],\n",
        "        \"provider_table\": provider_insert\n",
        "    }\n"
      ],
      "metadata": {
        "id": "YzxCh0Lz5MLL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}